# 🎉 Udiddit - Social News Aggregator Project 🎉

## 📊 Data Provided
A PostgreSQL database provided by Udacity contained the following two tables:

- 'bad_posts'

- 'bad_comments'

🏆 Project Results
The task given to me was divided into three parts:

Part I: 🕵️‍♂️ Investigate the Existing Schema
To start off, I delved into the current data model and thoroughly investigated it for any issues. The detailed tasks and instructions were found in the Google Docs document provided to me. I made a copy of the document to document my findings and steps.

Part II: 🛠️ Create the DDL for Your New Schema
Once I had a clear understanding of the current data model's flaws, the next step was to design and create a new schema for the Udiddit system. This new schema addressed the identified issues and aimed to make the data model more robust and efficient.

Part III: 🚀 Migrate the Provided Data
After finalizing the new schema, I proceeded to migrate the existing data from the provided SQL Workspace (bad_db.sql) into the new schema. This ensured that the system continued to function seamlessly with the updated data model.

📚 Supporting Materials
The project also includes some supporting materials to aid me in completing the tasks:

Google Docs Template: This document contains detailed instructions and tasks for the project. I have already created a copy of this document to record my progress and findings.

bad_db.sql: This file contains the SQL dump of the current data gathered by Udiddit. I can use this data to test my queries in the SQL Workspace or any other environment as required.

DDL Script: This script creates the new database with normalized tables.

DML Script: This script migrates data from the old database to the new normalized database.
