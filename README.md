# ğŸ‰ Udiddit - Social News Aggregator Project ğŸ‰

## ğŸ“Š Data Provided
A PostgreSQL database provided by Udacity contained the following two tables:

- 'bad_posts'

- 'bad_comments'

ğŸ† Project Results
The task given to me was divided into three parts:

Part I: ğŸ•µï¸â€â™‚ï¸ Investigate the Existing Schema
To start off, I delved into the current data model and thoroughly investigated it for any issues. The detailed tasks and instructions were found in the Google Docs document provided to me. I made a copy of the document to document my findings and steps.

Part II: ğŸ› ï¸ Create the DDL for Your New Schema
Once I had a clear understanding of the current data model's flaws, the next step was to design and create a new schema for the Udiddit system. This new schema addressed the identified issues and aimed to make the data model more robust and efficient.

Part III: ğŸš€ Migrate the Provided Data
After finalizing the new schema, I proceeded to migrate the existing data from the provided SQL Workspace (bad_db.sql) into the new schema. This ensured that the system continued to function seamlessly with the updated data model.

ğŸ“š Supporting Materials
The project also includes some supporting materials to aid me in completing the tasks:

Google Docs Template: This document contains detailed instructions and tasks for the project. I have already created a copy of this document to record my progress and findings.

bad_db.sql: This file contains the SQL dump of the current data gathered by Udiddit. I can use this data to test my queries in the SQL Workspace or any other environment as required.

DDL Script: This script creates the new database with normalized tables.

DML Script: This script migrates data from the old database to the new normalized database.
